{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1GvZqTgHacwr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19398,
     "status": "ok",
     "timestamp": 1750817582809,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "1GvZqTgHacwr",
    "outputId": "1688b73a-534b-4b74-ebc0-35fb27ac2d6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "H-LdqXLabG_G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1750817666609,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "H-LdqXLabG_G",
    "outputId": "249846da-d9e5-4428-9e9f-74d54f583354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/nlp/quora_data\n"
     ]
    }
   ],
   "source": [
    "cd  /content/drive/MyDrive/nlp/quora_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enExXmf4asvt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1750817668220,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "enExXmf4asvt",
    "outputId": "09dc5b74-2b31-48c5-8fa8-e66a8579e7f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_dataset.csv  semantic_web.csv  TextProcessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xwlW4Hfxcd4F",
   "metadata": {
    "id": "xwlW4Hfxcd4F"
   },
   "source": [
    "## expand_contractions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b7df32",
   "metadata": {
    "id": "80b7df32"
   },
   "outputs": [],
   "source": [
    "contractions_mapping = {\n",
    "    \"ain't\": [\"am\", \"not\"],\n",
    "    \"aren't\": [\"are\", \"not\"],\n",
    "    \"can't\": [\"can\", \"not\"],\n",
    "    \"can't've\": [\"cannot\", \"have\"],\n",
    "    \"'cause\": [\"because\"],\n",
    "    \"could've\": [\"could\", \"have\"],\n",
    "    \"couldn't\": [\"could\", \"not\"],\n",
    "    \"couldn't've\": [\"could\", \"not\", \"have\"],\n",
    "    \"didn't\": [\"did\", \"not\"],\n",
    "    \"doesn't\": [\"does\", \"not\"],\n",
    "    \"don't\": [\"do\", \"not\"],\n",
    "    \"hadn't\": [\"had\", \"not\"],\n",
    "    \"hadn't've\": [\"had\", \"not\", \"have\"],\n",
    "    \"hasn't\": [\"has\", \"not\"],\n",
    "    \"haven't\": [\"have\", \"not\"],\n",
    "    \"he'd\": [\"he\", \"would\"],\n",
    "    \"he'd've\": [\"he\", \"would\", \"have\"],\n",
    "    \"he'll\": [\"he\", \"will\"],\n",
    "    \"he'll've\": [\"he\", \"will\", \"have\"],\n",
    "    \"he's\": [\"he\", \"is\"],\n",
    "    \"how'd\": [\"how\", \"did\"],\n",
    "    \"how'd'y\": [\"how\", \"do\", \"you\"],\n",
    "    \"how'll\": [\"how\", \"will\"],\n",
    "    \"how's\": [\"how\", \"is\"],\n",
    "    \"i'd\": [\"i\", \"would\"],\n",
    "    \"i'd've\": [\"i\", \"would\", \"have\"],\n",
    "    \"i'll\": [\"i\", \"will\"],\n",
    "    \"i'll've\": [\"i\", \"will\", \"have\"],\n",
    "    \"i'm\": [\"i\", \"am\"],\n",
    "    \"i've\": [\"i\", \"have\"],\n",
    "    \"isn't\": [\"is\", \"not\"],\n",
    "    \"it'd\": [\"it\", \"would\"],\n",
    "    \"it'd've\": [\"it\", \"would\", \"have\"],\n",
    "    \"it'll\": [\"it\", \"will\"],\n",
    "    \"it'll've\": [\"it\", \"will\", \"have\"],\n",
    "    \"it's\": [\"it\", \"is\"],\n",
    "    \"let's\": [\"let\", \"us\"],\n",
    "    \"ma'am\": [\"madam\"],\n",
    "    \"mayn't\": [\"may\", \"not\"],\n",
    "    \"might've\": [\"might\", \"have\"],\n",
    "    \"mightn't\": [\"might\", \"not\"],\n",
    "    \"mightn't've\": [\"might\", \"not\", \"have\"],\n",
    "    \"must've\": [\"must\", \"have\"],\n",
    "    \"mustn't\": [\"must\", \"not\"],\n",
    "    \"mustn't've\": [\"must\", \"not\", \"have\"],\n",
    "    \"needn't\": [\"need\", \"not\"],\n",
    "    \"needn't've\": [\"need\", \"not\", \"have\"],\n",
    "    \"o'clock\": [\"of\", \"the\", \"clock\"],\n",
    "    \"oughtn't\": [\"ought\", \"not\"],\n",
    "    \"oughtn't've\": [\"ought\", \"not\", \"have\"],\n",
    "    \"shan't\": [\"shall\", \"not\"],\n",
    "    \"sha'n't\": [\"shall\", \"not\"],\n",
    "    \"shan't've\": [\"shall\", \"not\", \"have\"],\n",
    "    \"she'd\": [\"she\", \"would\"],\n",
    "    \"she'd've\": [\"she\", \"would\", \"have\"],\n",
    "    \"she'll\": [\"she\", \"will\"],\n",
    "    \"she'll've\": [\"she\", \"will\", \"have\"],\n",
    "    \"she's\": [\"she\", \"is\"],\n",
    "    \"should_ve\": [\"should\", \"have\"],\n",
    "    \"should've\": [\"should\", \"have\"],\n",
    "    \"shouldn't\": [\"should\", \"not\"],\n",
    "    \"shouldn't've\": [\"should\", \"not\", ],\n",
    "    \"shouldn't_ve\": [\"should\", \"not\", \"have\"],\n",
    "    \"so've\": [\"so\", \"have\"],\n",
    "    \"so's\": [\"so\", \"is\"],\n",
    "    \"that'd\": [\"that\", \"would\"],\n",
    "    \"that'd've\": [\"that\", \"would\", \"have\"],\n",
    "    \"that's\": [\"that\", \"is\"],\n",
    "    \"there'd\": [\"there\", \"would\"],\n",
    "    \"there'd've\": [\"there\", \"would\", \"have\"],\n",
    "    \"there's\": [\"there\", \"is\"],\n",
    "    \"they'd\": [\"they\", \"would\"],\n",
    "    \"they'd've\": [\"they\", \"would\", \"have\"],\n",
    "    \"they'll\": [\"they\", \"will\"],\n",
    "    \"they'll've\": [\"they\", \"will\", \"have\"],\n",
    "    \"they're\": [\"they\", \"are\"],\n",
    "    \"they've\": [\"they\", \"have\"],\n",
    "    \"to've\": [\"to\", \"have\"],\n",
    "    \"wasn't\": [\"was\", \"not\"],\n",
    "    \"we'd\": [\"we\", \"would\"],\n",
    "    \"we'd've\": [\"we\", \"would\", \"have\"],\n",
    "    \"we'll\": [\"we\", \"will\"],\n",
    "    \"we'll've\": [\"we\", \"will\", \"have\"],\n",
    "    \"we're\": [\"we\", \"are\"],\n",
    "    \"we've\": [\"we\", \"have\"],\n",
    "    \"weren't\": [\"were\", \"not\"],\n",
    "    \"what'll\": [\"what\", \"will\"],\n",
    "    \"what'll've\": [\"what\", \"will\", \"have\"],\n",
    "    \"what're\": [\"what\", \"are\"],\n",
    "    \"what's\": [\"what\", \"is\"],\n",
    "    \"what've\": [\"what\", \"have\"],\n",
    "    \"when's\": [\"when\", \"is\"],\n",
    "    \"when've\": [\"when\", \"have\"],\n",
    "    \"where'd\": [\"where\", \"did\"],\n",
    "    \"where's\": [\"where\", \"is\"],\n",
    "    \"where've\": [\"where\", \"have\"],\n",
    "    \"who'll\": [\"who\", \"will\"],\n",
    "    \"who'll've\": [\"who\", \"will\", \"have\"],\n",
    "    \"who's\": [\"who\", \"is\"],\n",
    "    \"who've\": [\"who\", \"have\"],\n",
    "    \"why's\": [\"why\", \"is\"],\n",
    "    \"why've\": [\"why\", \"have\"],\n",
    "    \"will've\": [\"will\", \"have\"],\n",
    "    \"won't\": [\"will\", \"not\"],\n",
    "    \"won't've\": [\"will\", \"not\", \"have\"],\n",
    "    \"would've\": [\"would\", \"have\"],\n",
    "    \"wouldn't\": [\"would\", \"not\"],\n",
    "    \"wouldn't've\": [\"would\", \"not\", \"have\"],\n",
    "    \"y'all\": [\"you\", \"all\"],\n",
    "    \"y'all'd\": [\"you\", \"all\", \"would\"],\n",
    "    \"y'all'd've\": [\"you\", \"all\", \"would\", \"have\"],\n",
    "    \"y'all're\": [\"you\", \"all\", \"are\"],\n",
    "    \"y'all've\": [\"you\", \"all\", \"have\"],\n",
    "    \"you'd\": [\"you\", \"would\"],\n",
    "    \"you'd've\": [\"you\", \"would\", \"have\"],\n",
    "    \"you'll\": [\"you\", \"will\"],\n",
    "    \"you'll've\": [\"you\", \"will\", \"have\"],\n",
    "    \"you're\": [\"you\", \"are\"],\n",
    "    \"you've\": [\"you\", \"have\"],\n",
    "}\n",
    "def expand_contractions(text):\n",
    "    expanded_text = []\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in contractions_mapping:\n",
    "            expanded_text.extend(contractions_mapping[word])\n",
    "        else:\n",
    "            expanded_text.append(word)\n",
    "    return \" \".join(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M0msY2jhc8hG",
   "metadata": {
    "id": "M0msY2jhc8hG"
   },
   "source": [
    "## remove_punctuations(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94b74e5",
   "metadata": {
    "id": "f94b74e5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_punctuations(text):\n",
    "    pattern = r'[.,!?:;\\-_()\\[\\]{}\"\\`~@#$%^&*=+|\\\\/<>‘]'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0QfC4x7dDhZ",
   "metadata": {
    "id": "v0QfC4x7dDhZ"
   },
   "source": [
    "## remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba65b62",
   "metadata": {
    "id": "0ba65b62"
   },
   "outputs": [],
   "source": [
    "stop_words = {\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n",
    "    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n",
    "    'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',\n",
    "    'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with',\n",
    "    'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
    "    'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
    "    'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n",
    "    'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',\n",
    "    'very', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "}\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UX8vFm-vdLOe",
   "metadata": {
    "id": "UX8vFm-vdLOe"
   },
   "source": [
    "## Text_Processing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ec2274",
   "metadata": {
    "id": "a9ec2274"
   },
   "outputs": [],
   "source": [
    "def Text_Processing(text):\n",
    "    text = text if isinstance(text, str) else str(text)\n",
    "    processedText = text.lower()\n",
    "    processedText = expand_contractions(processedText)\n",
    "    processedText = remove_punctuations(processedText)\n",
    "    processedText = remove_stopwords(processedText)\n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XbjtazL2lp8F",
   "metadata": {
    "id": "XbjtazL2lp8F"
   },
   "source": [
    "## Porter2(Snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "LOtcWxs9lo3k",
   "metadata": {
    "id": "LOtcWxs9lo3k"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Porter2Stemmer:\n",
    "    def __init__(self):\n",
    "        self.vowels = \"aeiouy\"\n",
    "        self.double_consonants = [\"bb\", \"dd\", \"ff\", \"gg\", \"mm\", \"nn\", \"pp\", \"rr\", \"tt\"]\n",
    "        self.li_endings = \"cdeghkmnrt\"\n",
    "\n",
    "        # Step 1a suffixes\n",
    "        self.step1a_suffixes = [\n",
    "            (\"sses\", \"ss\"),\n",
    "            (\"ied\", \"i\"),\n",
    "            (\"ies\", \"i\"),\n",
    "            (\"us\", \"us\"),\n",
    "            (\"ss\", \"ss\"),\n",
    "            (\"s\", \"\")\n",
    "        ]\n",
    "\n",
    "        # Step 1b suffixes\n",
    "        self.step1b_suffixes = [\n",
    "            \"eedly\", \"eed\", \"ingly\", \"edly\", \"ing\", \"ed\"\n",
    "        ]\n",
    "\n",
    "        # Step 2 suffixes\n",
    "        self.step2_suffixes = [\n",
    "            (\"ational\", \"ate\"),\n",
    "            (\"tional\", \"tion\"),\n",
    "            (\"enci\", \"ence\"),\n",
    "            (\"anci\", \"ance\"),\n",
    "            (\"abli\", \"able\"),\n",
    "            (\"entli\", \"ent\"),\n",
    "            (\"izer\", \"ize\"),\n",
    "            (\"ization\", \"ize\"),\n",
    "            (\"ation\", \"ate\"),\n",
    "            (\"ator\", \"ate\"),\n",
    "            (\"alism\", \"al\"),\n",
    "            (\"aliti\", \"al\"),\n",
    "            (\"alli\", \"al\"),\n",
    "            (\"fulness\", \"ful\"),\n",
    "            (\"ousli\", \"ous\"),\n",
    "            (\"ousness\", \"ous\"),\n",
    "            (\"iveness\", \"ive\"),\n",
    "            (\"iviti\", \"ive\"),\n",
    "            (\"biliti\", \"ble\"),\n",
    "            (\"bli\", \"ble\"),\n",
    "            (\"ogi\", \"og\"),\n",
    "            (\"fulli\", \"ful\"),\n",
    "            (\"lessli\", \"less\"),\n",
    "            (\"li\", \"\")\n",
    "        ]\n",
    "\n",
    "        # Step 3 suffixes\n",
    "        self.step3_suffixes = [\n",
    "            (\"ational\", \"ate\"),\n",
    "            (\"tional\", \"tion\"),\n",
    "            (\"alize\", \"al\"),\n",
    "            (\"icate\", \"ic\"),\n",
    "            (\"itive\", \"\"),\n",
    "            (\"ical\", \"ic\"),\n",
    "            (\"ful\", \"\"),\n",
    "            (\"ness\", \"\")\n",
    "        ]\n",
    "\n",
    "        # Step 4 suffixes\n",
    "        self.step4_suffixes = [\n",
    "            \"al\", \"ance\", \"ence\", \"er\", \"ic\", \"able\", \"ible\", \"ant\", \"ement\",\n",
    "            \"ment\", \"ent\", \"ion\", \"ou\", \"ism\", \"ate\", \"iti\", \"ous\", \"ive\", \"ize\"\n",
    "        ]\n",
    "\n",
    "        # Step 5 suffixes\n",
    "        self.step5_suffixes = [\"e\", \"l\"]\n",
    "\n",
    "    def is_vowel(self, word, i):\n",
    "        \"\"\"Check if character at position i is a vowel\"\"\"\n",
    "        if i < 0 or i >= len(word):\n",
    "            return False\n",
    "\n",
    "        char = word[i].lower()\n",
    "        if char in \"aeiou\":\n",
    "            return True\n",
    "        if char == 'y' and i > 0 and not self.is_vowel(word, i - 1):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_measure(self, word):\n",
    "        \"\"\"Calculate the measure of a word (number of VC patterns)\"\"\"\n",
    "        measure = 0\n",
    "        i = 0\n",
    "        word_len = len(word)\n",
    "\n",
    "        # Skip initial consonants\n",
    "        while i < word_len and not self.is_vowel(word, i):\n",
    "            i += 1\n",
    "\n",
    "        # Count VC patterns\n",
    "        while i < word_len:\n",
    "            # Skip vowels\n",
    "            while i < word_len and self.is_vowel(word, i):\n",
    "                i += 1\n",
    "            if i >= word_len:\n",
    "                break\n",
    "\n",
    "            # Skip consonants\n",
    "            while i < word_len and not self.is_vowel(word, i):\n",
    "                i += 1\n",
    "            measure += 1\n",
    "\n",
    "        return measure\n",
    "\n",
    "    def has_vowel(self, word):\n",
    "        \"\"\"Check if word contains a vowel\"\"\"\n",
    "        for i in range(len(word)):\n",
    "            if self.is_vowel(word, i):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def ends_with_double_consonant(self, word):\n",
    "        \"\"\"Check if word ends with double consonant\"\"\"\n",
    "        if len(word) >= 2:\n",
    "            last_two = word[-2:].lower()\n",
    "            return last_two in self.double_consonants\n",
    "        return False\n",
    "\n",
    "    def cvc_pattern(self, word):\n",
    "        \"\"\"Check if word ends with consonant-vowel-consonant pattern (with restrictions)\"\"\"\n",
    "        if len(word) < 3:\n",
    "            return False\n",
    "\n",
    "        word_len = len(word)\n",
    "        if (not self.is_vowel(word, word_len - 3) and\n",
    "            self.is_vowel(word, word_len - 2) and\n",
    "            not self.is_vowel(word, word_len - 1) and\n",
    "            word[-1].lower() not in \"wxy\"):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def find_r1_r2(self, word):\n",
    "        \"\"\"Find R1 and R2 regions\"\"\"\n",
    "        word_lower = word.lower()\n",
    "\n",
    "        # Special cases for R1\n",
    "        if word_lower.startswith((\"gener\", \"commun\", \"arsen\")):\n",
    "            r1_start = 5\n",
    "        else:\n",
    "            # Find first vowel followed by consonant\n",
    "            r1_start = 0\n",
    "            for i in range(len(word) - 1):\n",
    "                if self.is_vowel(word, i) and not self.is_vowel(word, i + 1):\n",
    "                    r1_start = i + 2\n",
    "                    break\n",
    "            else:\n",
    "                r1_start = len(word)\n",
    "\n",
    "        # R1 is the substring after the first vowel-consonant pair\n",
    "        r1 = word[r1_start:] if r1_start < len(word) else \"\"\n",
    "\n",
    "        # Find R2 within R1\n",
    "        r2_start = r1_start\n",
    "        for i in range(r1_start, len(word) - 1):\n",
    "            if self.is_vowel(word, i) and not self.is_vowel(word, i + 1):\n",
    "                r2_start = i + 2\n",
    "                break\n",
    "        else:\n",
    "            r2_start = len(word)\n",
    "\n",
    "        r2 = word[r2_start:] if r2_start < len(word) else \"\"\n",
    "\n",
    "        return r1_start, r2_start, r1, r2\n",
    "\n",
    "    def step0(self, word):\n",
    "        \"\"\"Step 0: Remove possessive suffixes\"\"\"\n",
    "        if word.endswith(\"'s'\"):\n",
    "            return word[:-3]\n",
    "        elif word.endswith(\"'s\"):\n",
    "            return word[:-2]\n",
    "        elif word.endswith(\"'\"):\n",
    "            return word[:-1]\n",
    "        return word\n",
    "\n",
    "    def step1a(self, word):\n",
    "        \"\"\"Step 1a: Handle plurals and past participles\"\"\"\n",
    "        for suffix, replacement in self.step1a_suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                if suffix in [\"ied\", \"ies\"]:\n",
    "                    if len(word) > 4:\n",
    "                        return word[:-len(suffix)] + \"i\"\n",
    "                    else:\n",
    "                        return word[:-len(suffix)] + \"ie\"\n",
    "                elif suffix == \"s\":\n",
    "                    # Only remove 's' if preceded by vowel that's not u\n",
    "                    if len(word) > 2:\n",
    "                        preceding = word[-2]\n",
    "                        if self.is_vowel(word, len(word) - 2) and preceding != 'u':\n",
    "                            return word[:-1]\n",
    "                    return word\n",
    "                else:\n",
    "                    return word[:-len(suffix)] + replacement\n",
    "        return word\n",
    "\n",
    "    def step1b(self, word):\n",
    "        \"\"\"Step 1b: Handle -ed, -edly, -ing, -ingly, -eed, -eedly\"\"\"\n",
    "        r1_start, r2_start, r1, r2 = self.find_r1_r2(word)\n",
    "\n",
    "        # Handle eedly and eed\n",
    "        if word.endswith(\"eedly\"):\n",
    "            if r1.endswith(\"eedly\"):\n",
    "                return word[:-5] + \"ee\"\n",
    "        elif word.endswith(\"eed\"):\n",
    "            if r1.endswith(\"eed\"):\n",
    "                return word[:-3] + \"ee\"\n",
    "\n",
    "        # Handle other suffixes\n",
    "        for suffix in [\"ingly\", \"edly\", \"ing\", \"ed\"]:\n",
    "            if word.endswith(suffix):\n",
    "                stem = word[:-len(suffix)]\n",
    "                if self.has_vowel(stem):\n",
    "                    # Post-process the stem\n",
    "                    if stem.endswith((\"at\", \"bl\", \"iz\")):\n",
    "                        return stem + \"e\"\n",
    "                    elif self.ends_with_double_consonant(stem):\n",
    "                        # Remove double consonant except for l, s, z\n",
    "                        if stem[-1].lower() not in \"lsz\":\n",
    "                            return stem[:-1]\n",
    "                    elif (self.get_measure(stem) == 1 and self.cvc_pattern(stem)):\n",
    "                        return stem + \"e\"\n",
    "                    return stem\n",
    "\n",
    "        return word\n",
    "\n",
    "    def step1c(self, word):\n",
    "        \"\"\"Step 1c: Replace y or Y with i if preceded by consonant\"\"\"\n",
    "        if len(word) > 2 and word[-1].lower() == 'y':\n",
    "            if not self.is_vowel(word, len(word) - 2):\n",
    "                return word[:-1] + 'i'\n",
    "        return word\n",
    "\n",
    "    def step2(self, word):\n",
    "        \"\"\"Step 2: Remove derivational suffixes\"\"\"\n",
    "        r1_start, r2_start, r1, r2 = self.find_r1_r2(word)\n",
    "\n",
    "        for suffix, replacement in self.step2_suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                if suffix == \"li\" and len(word) > 2:\n",
    "                    # Special case for li\n",
    "                    if r1.endswith(\"li\") and word[-3].lower() in self.li_endings:\n",
    "                        return word[:-2]\n",
    "                elif suffix == \"ogi\" and len(word) > 3:\n",
    "                    # Special case for ogi\n",
    "                    if r1.endswith(\"ogi\") and word[-4].lower() == 'l':\n",
    "                        return word[:-3] + \"og\"\n",
    "                elif r1.endswith(suffix):\n",
    "                    return word[:-len(suffix)] + replacement\n",
    "                break\n",
    "\n",
    "        return word\n",
    "\n",
    "    def step3(self, word):\n",
    "        \"\"\"Step 3: Remove derivational suffixes\"\"\"\n",
    "        r1_start, r2_start, r1, r2 = self.find_r1_r2(word)\n",
    "\n",
    "        for suffix, replacement in self.step3_suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                if suffix == \"ative\":\n",
    "                    if r2.endswith(\"ative\"):\n",
    "                        return word[:-6]\n",
    "                elif r1.endswith(suffix):\n",
    "                    return word[:-len(suffix)] + replacement\n",
    "                break\n",
    "\n",
    "        return word\n",
    "\n",
    "    def step4(self, word):\n",
    "        \"\"\"Step 4: Remove derivational suffixes\"\"\"\n",
    "        r1_start, r2_start, r1, r2 = self.find_r1_r2(word)\n",
    "\n",
    "        for suffix in self.step4_suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                if suffix == \"ion\":\n",
    "                    # Special case for ion\n",
    "                    if (r2.endswith(\"ion\") and len(word) > 3 and\n",
    "                        word[-4].lower() in \"st\"):\n",
    "                        return word[:-3]\n",
    "                elif r2.endswith(suffix):\n",
    "                    return word[:-len(suffix)]\n",
    "                break\n",
    "\n",
    "        return word\n",
    "\n",
    "    def step5(self, word):\n",
    "        \"\"\"Step 5: Remove e or l\"\"\"\n",
    "        r1_start, r2_start, r1, r2 = self.find_r1_r2(word)\n",
    "\n",
    "        # Step 5a: Remove e\n",
    "        if word.endswith(\"e\"):\n",
    "            if (r2.endswith(\"e\") or\n",
    "                (r1.endswith(\"e\") and not self.cvc_pattern(word[:-1]))):\n",
    "                return word[:-1]\n",
    "\n",
    "        # Step 5b: Remove l\n",
    "        if (word.endswith(\"l\") and r2.endswith(\"l\") and\n",
    "            len(word) > 1 and word[-2].lower() == 'l'):\n",
    "            return word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        \"\"\"Apply the complete Porter2 stemming algorithm\"\"\"\n",
    "        if len(word) <= 2:\n",
    "            return word\n",
    "\n",
    "        # Convert to lowercase for processing but preserve original case pattern\n",
    "        original = word\n",
    "        word = word.lower()\n",
    "\n",
    "        # Mark initial y as consonant by converting to Y\n",
    "        if word.startswith('y'):\n",
    "            word = 'Y' + word[1:]\n",
    "\n",
    "        # Mark y as consonant when preceded by vowel\n",
    "        new_word = \"\"\n",
    "        for i, char in enumerate(word):\n",
    "            if char == 'y' and i > 0 and self.is_vowel(word, i - 1):\n",
    "                new_word += 'Y'\n",
    "            else:\n",
    "                new_word += char\n",
    "        word = new_word\n",
    "\n",
    "        # Apply stemming steps\n",
    "        word = self.step0(word)\n",
    "        word = self.step1a(word)\n",
    "        word = self.step1b(word)\n",
    "        word = self.step1c(word)\n",
    "        word = self.step2(word)\n",
    "        word = self.step3(word)\n",
    "        word = self.step4(word)\n",
    "        word = self.step5(word)\n",
    "\n",
    "        # Convert Y back to y\n",
    "        word = word.replace('Y', 'y')\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qR4Zuox92HRf",
   "metadata": {
    "id": "qR4Zuox92HRf"
   },
   "source": [
    "## text_stemmer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "TCN-PKhx2PwR",
   "metadata": {
    "id": "TCN-PKhx2PwR"
   },
   "outputs": [],
   "source": [
    "stemmer = Porter2Stemmer()\n",
    "def text_stemmer(text):\n",
    "    text = text if isinstance(text, str) else str(text)\n",
    "\n",
    "    words = text.strip().split()\n",
    "\n",
    "    # If it's a single word, stem and return it\n",
    "    if len(words) == 1:\n",
    "        return stemmer.stem(words[0])\n",
    "\n",
    "    # If it's a sentence, stem each word\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zPYP8sVVdfEh",
   "metadata": {
    "id": "zPYP8sVVdfEh"
   },
   "source": [
    "## build_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be3c5483",
   "metadata": {
    "id": "be3c5483"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocabulary(dataset):\n",
    "    word_counter = Counter()\n",
    "    for sentence in dataset:\n",
    "        words = Text_Processing(sentence).split()\n",
    "        word_counter.update(words)\n",
    "\n",
    "    sorted_words = sorted(word_counter.items(), key=lambda x: x[1])\n",
    "\n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(sorted_words)}\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ICogm_bnfKla",
   "metadata": {
    "id": "ICogm_bnfKla"
   },
   "source": [
    "## remove unwanted samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e1348a",
   "metadata": {
    "id": "82e1348a"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def remove_samples_with_uncommon_chars(df, col_list=['question1', 'question2']):\n",
    "    allowed_pattern = re.compile(r'^[a-zA-Z0-9\\s!\"#$%&\\'()*+,\\-./:;<=>?@\\\\^_`{|}~]*$')\n",
    "    def is_clean(text):\n",
    "        text = str(text)  # Ensure it's a string\n",
    "        return bool(allowed_pattern.fullmatch(text))\n",
    "    mask = df[col_list].map(is_clean).all(axis=1)\n",
    "    return df[mask].reset_index(drop=True)\n",
    "\n",
    "def remove_short_samples(df, col_list=['question1', 'question2'], min_words=6):\n",
    "    def word_count(text):\n",
    "        return len(str(text).split())\n",
    "    mask = df[col_list].map(word_count).gt(min_words - 1).all(axis=1)\n",
    "    return df[mask].reset_index(drop=True)\n",
    "\n",
    "def remove_rare_word_samples(df, col_list=['question1', 'question2'], min_freq=5):\n",
    "    all_words = []\n",
    "    for col in col_list:\n",
    "        all_words.extend(' '.join(df[col].astype(str)).split())\n",
    "    word_freq = Counter(all_words)\n",
    "    def has_only_frequent_words(text):\n",
    "        words = str(text).split()\n",
    "        return all(word_freq[word] >= min_freq for word in words)\n",
    "    mask = df[col_list].map(has_only_frequent_words).all(axis=1)\n",
    "    return df[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7BFL4xfHdpUI",
   "metadata": {
    "id": "7BFL4xfHdpUI"
   },
   "source": [
    "## processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700331bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2466,
     "status": "ok",
     "timestamp": 1750798435570,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "700331bb",
    "outputId": "882f3fff-7a4e-4127-9907-5883f6e2fff4"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'semantic_web.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m dataset = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msemantic_web.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\IBRAHIM_ALAHMAR\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\IBRAHIM_ALAHMAR\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\IBRAHIM_ALAHMAR\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\IBRAHIM_ALAHMAR\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\IBRAHIM_ALAHMAR\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'semantic_web.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('semantic_web.csv')\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51d69f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1268,
     "status": "ok",
     "timestamp": 1750798436841,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "5b51d69f",
    "outputId": "d7f79cac-016e-4c37-b066-5ad92c944238"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394626"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncommon = remove_samples_with_uncommon_chars(dataset)\n",
    "len(uncommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fm8CyKfm5p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1750798437881,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "31fm8CyKfm5p",
    "outputId": "f5f19fa4-f37b-4e1a-9fbe-a577d3f839f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343795"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_samples = remove_short_samples(uncommon)\n",
    "len(short_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5Nhu-Uc7gbfK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6955,
     "status": "ok",
     "timestamp": 1750798444846,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "5Nhu-Uc7gbfK",
    "outputId": "5d72f6dc-50f4-47f8-c373-fa65f80a1a80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182832"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rare_words = remove_rare_word_samples(short_samples, min_freq=10)\n",
    "len(rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PEQtL7WOg5GV",
   "metadata": {
    "id": "PEQtL7WOg5GV"
   },
   "outputs": [],
   "source": [
    "questions1 = rare_words['question1']\n",
    "\n",
    "questions2 = rare_words['question2']\n",
    "\n",
    "is_duplicate = rare_words['is_duplicate']\n",
    "\n",
    "new_dataset = pd.DataFrame({\n",
    "    'question1': questions1,\n",
    "    'question2': questions2,\n",
    "    'is_duplicate': is_duplicate\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "new_dataset.to_csv(\"processed_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ctMkI2-hmVRh",
   "metadata": {
    "id": "ctMkI2-hmVRh"
   },
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1v__-TCD1Oda",
   "metadata": {
    "id": "1v__-TCD1Oda"
   },
   "source": [
    "## building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PEnrOp9FiN_U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1750801637460,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "PEnrOp9FiN_U",
    "outputId": "a7896138-b8b2-4c14-fbb3-5bfd0c92d951"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182832"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('processed_dataset.csv')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EtFbFjgMi4LF",
   "metadata": {
    "id": "EtFbFjgMi4LF"
   },
   "outputs": [],
   "source": [
    "Quest1 = dataset['question1']\n",
    "Quest2 = dataset['question2']\n",
    "Is_duplicate = dataset['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HGTyv5ApjW2a",
   "metadata": {
    "id": "HGTyv5ApjW2a"
   },
   "outputs": [],
   "source": [
    "combined_dataset = list(Quest1) + list(Quest2)\n",
    "vocabulary = build_vocabulary(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_p96dP4DkIzh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1750802772066,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "_p96dP4DkIzh",
    "outputId": "40f7573c-47b2-4ff9-c4c4-910ecf766d50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15881"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KsNBhpG36SVq",
   "metadata": {
    "id": "KsNBhpG36SVq"
   },
   "outputs": [],
   "source": [
    "vocabulary = [stemmer.stem(voc) for voc in vocabulary]\n",
    "vocabulary = list(dict.fromkeys(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5CGpo8R67aB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1750807122351,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "a5CGpo8R67aB",
    "outputId": "c093527e-c87e-4f1f-8a7b-bd5382190dcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12050"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CdEs_zzU1btl",
   "metadata": {
    "id": "CdEs_zzU1btl"
   },
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2zie3hDs-99X",
   "metadata": {
    "id": "2zie3hDs-99X"
   },
   "outputs": [],
   "source": [
    "def bag_of_words(vocabulary, sentence):\n",
    "    # Tokenize the sentence\n",
    "    sentence = Text_Processing(sentence)\n",
    "    sentence = stemmer.stem(sentence)\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Initialize binary BoW vector\n",
    "    bow_vector = [0] * len(vocabulary)\n",
    "\n",
    "    # Set 1 if the word exists in the sentence\n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        bow_vector[idx] = 1 if word in words else 0\n",
    "\n",
    "    return bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M0GN3SzTJykP",
   "metadata": {
    "id": "M0GN3SzTJykP"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "def predict(x, weights, bias):\n",
    "    z = sum(w * xi for w, xi in zip(weights, x)) + bias\n",
    "    return sigmoid(z)\n",
    "\n",
    "def train_logistic_regression(X, y, lr=0.1, epochs=100):\n",
    "    n_features = len(X[0])\n",
    "    weights = [random.uniform(-0.01, 0.01) for _ in range(n_features)]\n",
    "    bias = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for xi, yi in zip(X, y):\n",
    "            y_hat = predict(xi, weights, bias)\n",
    "            error = y_hat - yi\n",
    "            # Gradient update\n",
    "            for i in range(n_features):\n",
    "                weights[i] -= lr * error * xi[i]\n",
    "            bias -= lr * error\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "def classify(x, weights, bias):\n",
    "    return int(predict(x, weights, bias) >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_-2VzDRsUqMH",
   "metadata": {
    "id": "_-2VzDRsUqMH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dkYFQCZUUVf8",
   "metadata": {
    "id": "dkYFQCZUUVf8"
   },
   "outputs": [],
   "source": [
    "lr_weights, lr_bias = train_logistic_regression(X, y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xwlW4Hfxcd4F",
    "M0msY2jhc8hG",
    "v0QfC4x7dDhZ",
    "UX8vFm-vdLOe",
    "XbjtazL2lp8F",
    "qR4Zuox92HRf",
    "zPYP8sVVdfEh",
    "ICogm_bnfKla",
    "7BFL4xfHdpUI"
   ],
   "provenance": [
    {
     "file_id": "1mpct2jtoWO-QBNRmezllAkOD6ndfQg1G",
     "timestamp": 1750817538329
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
