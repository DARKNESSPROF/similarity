{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "xwlW4Hfxcd4F",
   "metadata": {
    "id": "xwlW4Hfxcd4F"
   },
   "source": [
    "## expand_contractions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b7df32",
   "metadata": {
    "id": "80b7df32"
   },
   "outputs": [],
   "source": [
    "contractions_mapping = {\n",
    "    \"ain't\": [\"am\", \"not\"],\n",
    "    \"aren't\": [\"are\", \"not\"],\n",
    "    \"can't\": [\"can\", \"not\"],\n",
    "    \"can't've\": [\"cannot\", \"have\"],\n",
    "    \"'cause\": [\"because\"],\n",
    "    \"could've\": [\"could\", \"have\"],\n",
    "    \"couldn't\": [\"could\", \"not\"],\n",
    "    \"couldn't've\": [\"could\", \"not\", \"have\"],\n",
    "    \"didn't\": [\"did\", \"not\"],\n",
    "    \"doesn't\": [\"does\", \"not\"],\n",
    "    \"don't\": [\"do\", \"not\"],\n",
    "    \"hadn't\": [\"had\", \"not\"],\n",
    "    \"hadn't've\": [\"had\", \"not\", \"have\"],\n",
    "    \"hasn't\": [\"has\", \"not\"],\n",
    "    \"haven't\": [\"have\", \"not\"],\n",
    "    \"he'd\": [\"he\", \"would\"],\n",
    "    \"he'd've\": [\"he\", \"would\", \"have\"],\n",
    "    \"he'll\": [\"he\", \"will\"],\n",
    "    \"he'll've\": [\"he\", \"will\", \"have\"],\n",
    "    \"he's\": [\"he\", \"is\"],\n",
    "    \"how'd\": [\"how\", \"did\"],\n",
    "    \"how'd'y\": [\"how\", \"do\", \"you\"],\n",
    "    \"how'll\": [\"how\", \"will\"],\n",
    "    \"how's\": [\"how\", \"is\"],\n",
    "    \"i'd\": [\"i\", \"would\"],\n",
    "    \"i'd've\": [\"i\", \"would\", \"have\"],\n",
    "    \"i'll\": [\"i\", \"will\"],\n",
    "    \"i'll've\": [\"i\", \"will\", \"have\"],\n",
    "    \"i'm\": [\"i\", \"am\"],\n",
    "    \"i've\": [\"i\", \"have\"],\n",
    "    \"isn't\": [\"is\", \"not\"],\n",
    "    \"it'd\": [\"it\", \"would\"],\n",
    "    \"it'd've\": [\"it\", \"would\", \"have\"],\n",
    "    \"it'll\": [\"it\", \"will\"],\n",
    "    \"it'll've\": [\"it\", \"will\", \"have\"],\n",
    "    \"it's\": [\"it\", \"is\"],\n",
    "    \"let's\": [\"let\", \"us\"],\n",
    "    \"ma'am\": [\"madam\"],\n",
    "    \"mayn't\": [\"may\", \"not\"],\n",
    "    \"might've\": [\"might\", \"have\"],\n",
    "    \"mightn't\": [\"might\", \"not\"],\n",
    "    \"mightn't've\": [\"might\", \"not\", \"have\"],\n",
    "    \"must've\": [\"must\", \"have\"],\n",
    "    \"mustn't\": [\"must\", \"not\"],\n",
    "    \"mustn't've\": [\"must\", \"not\", \"have\"],\n",
    "    \"needn't\": [\"need\", \"not\"],\n",
    "    \"needn't've\": [\"need\", \"not\", \"have\"],\n",
    "    \"o'clock\": [\"of\", \"the\", \"clock\"],\n",
    "    \"oughtn't\": [\"ought\", \"not\"],\n",
    "    \"oughtn't've\": [\"ought\", \"not\", \"have\"],\n",
    "    \"shan't\": [\"shall\", \"not\"],\n",
    "    \"sha'n't\": [\"shall\", \"not\"],\n",
    "    \"shan't've\": [\"shall\", \"not\", \"have\"],\n",
    "    \"she'd\": [\"she\", \"would\"],\n",
    "    \"she'd've\": [\"she\", \"would\", \"have\"],\n",
    "    \"she'll\": [\"she\", \"will\"],\n",
    "    \"she'll've\": [\"she\", \"will\", \"have\"],\n",
    "    \"she's\": [\"she\", \"is\"],\n",
    "    \"should_ve\": [\"should\", \"have\"],\n",
    "    \"should've\": [\"should\", \"have\"],\n",
    "    \"shouldn't\": [\"should\", \"not\"],\n",
    "    \"shouldn't've\": [\"should\", \"not\", ],\n",
    "    \"shouldn't_ve\": [\"should\", \"not\", \"have\"],\n",
    "    \"so've\": [\"so\", \"have\"],\n",
    "    \"so's\": [\"so\", \"is\"],\n",
    "    \"that'd\": [\"that\", \"would\"],\n",
    "    \"that'd've\": [\"that\", \"would\", \"have\"],\n",
    "    \"that's\": [\"that\", \"is\"],\n",
    "    \"there'd\": [\"there\", \"would\"],\n",
    "    \"there'd've\": [\"there\", \"would\", \"have\"],\n",
    "    \"there's\": [\"there\", \"is\"],\n",
    "    \"they'd\": [\"they\", \"would\"],\n",
    "    \"they'd've\": [\"they\", \"would\", \"have\"],\n",
    "    \"they'll\": [\"they\", \"will\"],\n",
    "    \"they'll've\": [\"they\", \"will\", \"have\"],\n",
    "    \"they're\": [\"they\", \"are\"],\n",
    "    \"they've\": [\"they\", \"have\"],\n",
    "    \"to've\": [\"to\", \"have\"],\n",
    "    \"wasn't\": [\"was\", \"not\"],\n",
    "    \"we'd\": [\"we\", \"would\"],\n",
    "    \"we'd've\": [\"we\", \"would\", \"have\"],\n",
    "    \"we'll\": [\"we\", \"will\"],\n",
    "    \"we'll've\": [\"we\", \"will\", \"have\"],\n",
    "    \"we're\": [\"we\", \"are\"],\n",
    "    \"we've\": [\"we\", \"have\"],\n",
    "    \"weren't\": [\"were\", \"not\"],\n",
    "    \"what'll\": [\"what\", \"will\"],\n",
    "    \"what'll've\": [\"what\", \"will\", \"have\"],\n",
    "    \"what're\": [\"what\", \"are\"],\n",
    "    \"what's\": [\"what\", \"is\"],\n",
    "    \"what've\": [\"what\", \"have\"],\n",
    "    \"when's\": [\"when\", \"is\"],\n",
    "    \"when've\": [\"when\", \"have\"],\n",
    "    \"where'd\": [\"where\", \"did\"],\n",
    "    \"where's\": [\"where\", \"is\"],\n",
    "    \"where've\": [\"where\", \"have\"],\n",
    "    \"who'll\": [\"who\", \"will\"],\n",
    "    \"who'll've\": [\"who\", \"will\", \"have\"],\n",
    "    \"who's\": [\"who\", \"is\"],\n",
    "    \"who've\": [\"who\", \"have\"],\n",
    "    \"why's\": [\"why\", \"is\"],\n",
    "    \"why've\": [\"why\", \"have\"],\n",
    "    \"will've\": [\"will\", \"have\"],\n",
    "    \"won't\": [\"will\", \"not\"],\n",
    "    \"won't've\": [\"will\", \"not\", \"have\"],\n",
    "    \"would've\": [\"would\", \"have\"],\n",
    "    \"wouldn't\": [\"would\", \"not\"],\n",
    "    \"wouldn't've\": [\"would\", \"not\", \"have\"],\n",
    "    \"y'all\": [\"you\", \"all\"],\n",
    "    \"y'all'd\": [\"you\", \"all\", \"would\"],\n",
    "    \"y'all'd've\": [\"you\", \"all\", \"would\", \"have\"],\n",
    "    \"y'all're\": [\"you\", \"all\", \"are\"],\n",
    "    \"y'all've\": [\"you\", \"all\", \"have\"],\n",
    "    \"you'd\": [\"you\", \"would\"],\n",
    "    \"you'd've\": [\"you\", \"would\", \"have\"],\n",
    "    \"you'll\": [\"you\", \"will\"],\n",
    "    \"you'll've\": [\"you\", \"will\", \"have\"],\n",
    "    \"you're\": [\"you\", \"are\"],\n",
    "    \"you've\": [\"you\", \"have\"],\n",
    "}\n",
    "def expand_contractions(text):\n",
    "    expanded_text = []\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in contractions_mapping:\n",
    "            expanded_text.extend(contractions_mapping[word])\n",
    "        else:\n",
    "            expanded_text.append(word)\n",
    "    return \" \".join(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M0msY2jhc8hG",
   "metadata": {
    "id": "M0msY2jhc8hG"
   },
   "source": [
    "## remove_punctuations(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94b74e5",
   "metadata": {
    "id": "f94b74e5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_punctuations(text):\n",
    "    pattern = r'[.,!?:;\\-_()\\[\\]{}\"\\`~@#$%^&*=+|\\\\/<>‘]'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0QfC4x7dDhZ",
   "metadata": {
    "id": "v0QfC4x7dDhZ"
   },
   "source": [
    "## remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba65b62",
   "metadata": {
    "id": "0ba65b62"
   },
   "outputs": [],
   "source": [
    "stop_words = {\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n",
    "    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n",
    "    'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',\n",
    "    'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with',\n",
    "    'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
    "    'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
    "    'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n",
    "    'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',\n",
    "    'very', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "}\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UX8vFm-vdLOe",
   "metadata": {
    "id": "UX8vFm-vdLOe"
   },
   "source": [
    "## Text_Processing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ec2274",
   "metadata": {
    "id": "a9ec2274"
   },
   "outputs": [],
   "source": [
    "def Text_Processing(text):\n",
    "    text = text if isinstance(text, str) else str(text)\n",
    "    processedText = text.lower()\n",
    "    processedText = expand_contractions(processedText)\n",
    "    processedText = remove_punctuations(processedText)\n",
    "    processedText = remove_stopwords(processedText)\n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XbjtazL2lp8F",
   "metadata": {
    "id": "XbjtazL2lp8F"
   },
   "source": [
    "## Porter2(Snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "LOtcWxs9lo3k",
   "metadata": {
    "id": "LOtcWxs9lo3k"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Porter2Stemmer:\n",
    "    def __init__(self):\n",
    "        self.vowels = \"aeiouy\"\n",
    "        self.double_consonants = [\"bb\", \"dd\", \"ff\", \"gg\", \"mm\", \"nn\", \"pp\", \"rr\", \"tt\"]\n",
    "        self.li_endings = \"cdeghkmnrt\"\n",
    "\n",
    "        # Step 1a suffixes\n",
    "        self.step1a_suffixes = [\n",
    "            (\"sses\", \"ss\"),\n",
    "            (\"ied\", \"i\"),\n",
    "            (\"ies\", \"i\"),\n",
    "            (\"us\", \"us\"),\n",
    "            (\"ss\", \"ss\"),\n",
    "            (\"s\", \"\")\n",
    "        ]\n",
    "\n",
    "        # Step 1b suffixes\n",
    "        self.step1b_suffixes = [\n",
    "            \"eedly\", \"eed\", \"ingly\", \"edly\", \"ing\", \"ed\"\n",
    "        ]\n",
    "\n",
    "        # Step 2 suffixes\n",
    "        self.step2_suffixes = [\n",
    "            (\"ational\", \"ate\"),\n",
    "            (\"tional\", \"tion\"),\n",
    "            (\"enci\", \"ence\"),\n",
    "            (\"anci\", \"ance\"),\n",
    "            (\"abli\", \"able\"),\n",
    "            (\"entli\", \"ent\"),\n",
    "            (\"izer\", \"ize\"),\n",
    "            (\"ization\", \"ize\"),\n",
    "            (\"ation\", \"ate\"),\n",
    "            (\"ator\", \"ate\"),\n",
    "            (\"alism\", \"al\"),\n",
    "            (\"aliti\", \"al\"),\n",
    "            (\"alli\", \"al\"),\n",
    "            (\"fulness\", \"ful\"),\n",
    "            (\"ousli\", \"ous\"),\n",
    "            (\"ousness\", \"ous\"),\n",
    "            (\"iveness\", \"ive\"),\n",
    "            (\"iviti\", \"ive\"),\n",
    "            (\"biliti\", \"ble\"),\n",
    "            (\"bli\", \"ble\"),\n",
    "            (\"ogi\", \"og\"),\n",
    "            (\"fulli\", \"ful\"),\n",
    "            (\"lessli\", \"less\"),\n",
    "            (\"li\", \"\")\n",
    "        ]\n",
    "\n",
    "        # Step 3 suffixes\n",
    "        self.step3_suffixes = [\n",
    "            (\"ational\", \"ate\"),\n",
    "            (\"tional\", \"tion\"),\n",
    "            (\"alize\", \"al\"),\n",
    "            (\"icate\", \"ic\"),\n",
    "            (\"itive\", \"\"),\n",
    "            (\"ical\", \"ic\"),\n",
    "            (\"ful\", \"\"),\n",
    "            (\"ness\", \"\")\n",
    "        ]\n",
    "\n",
    "        # Step 4 suffixes\n",
    "        self.step4_suffixes = [\n",
    "            \"al\", \"ance\", \"ence\", \"er\", \"ic\", \"able\", \"ible\", \"ant\", \"ement\",\n",
    "            \"ment\", \"ent\", \"ion\", \"ou\", \"ism\", \"ate\", \"iti\", \"ous\", \"ive\", \"ize\"\n",
    "        ]\n",
    "\n",
    "        # Step 5 suffixes\n",
    "        self.step5_suffixes = [\"e\", \"l\"]\n",
    "\n",
    "    def is_vowel(self, word, i):\n",
    "        \"\"\"Check if character at position i is a vowel\"\"\"\n",
    "        if i < 0 or i >= len(word):\n",
    "            return False\n",
    "\n",
    "        char = word[i].lower()\n",
    "        if char in \"aeiou\":\n",
    "            return True\n",
    "        if char == 'y' and i > 0 and not self.is_vowel(word, i - 1):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_measure(self, word):\n",
    "        \"\"\"Calculate the measure of a word (number of VC patterns)\"\"\"\n",
    "        measure = 0\n",
    "        i = 0\n",
    "        word_len = len(word)\n",
    "\n",
    "        # Skip initial consonants\n",
    "        while i < word_len and not self.is_vowel(word, i):\n",
    "            i += 1\n",
    "\n",
    "        # Count VC patterns\n",
    "        while i < word_len:\n",
    "            # Skip vowels\n",
    "            while i < word_len and self.is_vowel(word, i):\n",
    "                i += 1\n",
    "            if i >= word_len:\n",
    "                break\n",
    "\n",
    "            # Skip consonants\n",
    "            while i < word_len and not self.is_vowel(word, i):\n",
    "                i += 1\n",
    "            measure += 1\n",
    "\n",
    "        return measure\n",
    "\n",
    "    def has_vowel(self, word):\n",
    "        \"\"\"Check if word contains a vowel\"\"\"\n",
    "        for i in range(len(word)):\n",
    "            if self.is_vowel(word, i):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def ends_with_double_consonant(self, word):\n",
    "        \"\"\"Check if word ends with double consonant\"\"\"\n",
    "        if len(word) >= 2:\n",
    "            last_two = word[-2:].lower()\n",
    "            return last_two in self.double_consonants\n",
    "        return False\n",
    "\n",
    "    def cvc_pattern(self, word):\n",
    "        \"\"\"Check if word ends with consonant-vowel-consonant pattern (with restrictions)\"\"\"\n",
    "        if len(word) < 3:\n",
    "            return False\n",
    "\n",
    "        word_len = len(word)\n",
    "        if (not self.is_vowel(word, word_len - 3) and\n",
    "            self.is_vowel(word, word_len - 2) and\n",
    "            not self.is_vowel(word, word_len - 1) and\n",
    "            word[-1].lower() not in \"wxy\"):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def find_r1_r2(self, word):\n",
    "        \"\"\"Find R1 and R2 regions\"\"\"\n",
    "        word_lower = word.lower()\n",
    "\n",
    "        # Special cases for R1\n",
    "        if word_lower.startswith((\"gener\", \"commun\", \"arsen\")):\n",
    "            r1_start = 5\n",
    "        else:\n",
    "            # Find first vowel followed by consonant\n",
    "            r1_start = 0\n",
    "            for i in range(len(word) - 1):\n",
    "                if self.is_vowel(word, i) and not self.is_vowel(word, i + 1):\n",
    "                    r1_start = i + 2\n",
    "                    break\n",
    "            else:\n",
    "                r1_start = len(word)\n",
    "\n",
    "        # R1 is the substring after the first vowel-consonant pair\n",
    "        r1 = word[r1_start:] if r1_start < len(word) else \"\"\n",
    "\n",
    "        # Find R2 within R1\n",
    "        r2_start = r1_start\n",
    "        for i in range(r1_start, len(word) - 1):\n",
    "            if self.is_vowel(word, i) and not self.is_vowel(word, i + 1):\n",
    "                r2_start = i + 2\n",
    "                break\n",
    "        else:\n",
    "            r2_start = len(word)\n",
    "\n",
    "        r2 = word[r2_start:] if r2_start < len(word) else \"\"\n",
    "\n",
    "        return r1_start, r2_start, r1, r2\n",
    "\n",
    "    def step0(self, word):\n",
    "        \"\"\"Step 0: Remove possessive suffixes\"\"\"\n",
    "        if word.endswith(\"'s'\"):\n",
    "            return word[:-3]\n",
    "        elif word.endswith(\"'s\"):\n",
    "            return word[:-2]\n",
    "        elif word.endswith(\"'\"):\n",
    "            return word[:-1]\n",
    "        return word\n",
    "\n",
    "    def step1a(self, word):\n",
    "        \"\"\"Step 1a: Handle plurals and past participles\"\"\"\n",
    "        for suffix, replacement in self.step1a_suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                if suffix in [\"ied\", \"ies\"]:\n",
    "                    if len(word) > 4:\n",
    "                        return word[:-len(suffix)] + \"i\"\n",
    "                    else:\n",
    "                        return word[:-len(suffix)] + \"ie\"\n",
    "                elif suffix == \"s\":\n",
    "                    # Only remove 's' if preceded by vowel that's not u\n",
    "                    if len(word) > 2:\n",
    "                        preceding = word[-2]\n",
    "                        if self.is_vowel(word, len(word) - 2) and preceding != 'u':\n",
    "                            return word[:-1]\n",
    "                    return word\n",
    "                else:\n",
    "                    return word[:-len(suffix)] + replacement\n",
    "        return word\n",
    "\n",
    "    def step1b(self, word):\n",
    "        \"\"\"Step 1b: Handle -ed, -edly, -ing, -ingly, -eed, -eedly\"\"\"\n",
    "        r1_start, r2_start, r1, r2 = self.find_r1_r2(word)\n",
    "\n",
    "        # Handle eedly and eed\n",
    "        if word.endswith(\"eedly\"):\n",
    "            if r1.endswith(\"eedly\"):\n",
    "                return word[:-5] + \"ee\"\n",
    "        elif word.endswith(\"eed\"):\n",
    "            if r1.endswith(\"eed\"):\n",
    "                return word[:-3] + \"ee\"\n",
    "\n",
    "        # Handle other suffixes\n",
    "        for suffix in [\"ingly\", \"edly\", \"ing\", \"ed\"]:\n",
    "            if word.endswith(suffix):\n",
    "                stem = word[:-len(suffix)]\n",
    "                if self.has_vowel(stem):\n",
    "                    # Post-process the stem\n",
    "                    if stem.endswith((\"at\", \"bl\", \"iz\")):\n",
    "                        return stem + \"e\"\n",
    "                    elif self.ends_with_double_consonant(stem):\n",
    "                        # Remove double consonant except for l, s, z\n",
    "                        if stem[-1].lower() not in \"lsz\":\n",
    "                            return stem[:-1]\n",
    "                    elif (self.get_measure(stem) == 1 and self.cvc_pattern(stem)):\n",
    "                        return stem + \"e\"\n",
    "                    return stem\n",
    "\n",
    "        return word\n",
    "\n",
    "    def step1c(self, word):\n",
    "        \"\"\"Step 1c: Replace y or Y with i if preceded by consonant\"\"\"\n",
    "        if len(word) > 2 and word[-1].lower() == 'y':\n",
    "            if not self.is_vowel(word, len(word) - 2):\n",
    "                return word[:-1] + 'i'\n",
    "        return word\n",
    "\n",
    "    def step2(self, word):\n",
    "        \"\"\"Step 2: Remove derivational suffixes\"\"\"\n",
    "        r1_start, r2_start, r1, r2 = self.find_r1_r2(word)\n",
    "\n",
    "        for suffix, replacement in self.step2_suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                if suffix == \"li\" and len(word) > 2:\n",
    "                    # Special case for li\n",
    "                    if r1.endswith(\"li\") and word[-3].lower() in self.li_endings:\n",
    "                        return word[:-2]\n",
    "                elif suffix == \"ogi\" and len(word) > 3:\n",
    "                    # Special case for ogi\n",
    "                    if r1.endswith(\"ogi\") and word[-4].lower() == 'l':\n",
    "                        return word[:-3] + \"og\"\n",
    "                elif r1.endswith(suffix):\n",
    "                    return word[:-len(suffix)] + replacement\n",
    "                break\n",
    "\n",
    "        return word\n",
    "\n",
    "    def step3(self, word):\n",
    "        \"\"\"Step 3: Remove derivational suffixes\"\"\"\n",
    "        r1_start, r2_start, r1, r2 = self.find_r1_r2(word)\n",
    "\n",
    "        for suffix, replacement in self.step3_suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                if suffix == \"ative\":\n",
    "                    if r2.endswith(\"ative\"):\n",
    "                        return word[:-6]\n",
    "                elif r1.endswith(suffix):\n",
    "                    return word[:-len(suffix)] + replacement\n",
    "                break\n",
    "\n",
    "        return word\n",
    "\n",
    "    def step4(self, word):\n",
    "        \"\"\"Step 4: Remove derivational suffixes\"\"\"\n",
    "        r1_start, r2_start, r1, r2 = self.find_r1_r2(word)\n",
    "\n",
    "        for suffix in self.step4_suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                if suffix == \"ion\":\n",
    "                    # Special case for ion\n",
    "                    if (r2.endswith(\"ion\") and len(word) > 3 and\n",
    "                        word[-4].lower() in \"st\"):\n",
    "                        return word[:-3]\n",
    "                elif r2.endswith(suffix):\n",
    "                    return word[:-len(suffix)]\n",
    "                break\n",
    "\n",
    "        return word\n",
    "\n",
    "    def step5(self, word):\n",
    "        \"\"\"Step 5: Remove e or l\"\"\"\n",
    "        r1_start, r2_start, r1, r2 = self.find_r1_r2(word)\n",
    "\n",
    "        # Step 5a: Remove e\n",
    "        if word.endswith(\"e\"):\n",
    "            if (r2.endswith(\"e\") or\n",
    "                (r1.endswith(\"e\") and not self.cvc_pattern(word[:-1]))):\n",
    "                return word[:-1]\n",
    "\n",
    "        # Step 5b: Remove l\n",
    "        if (word.endswith(\"l\") and r2.endswith(\"l\") and\n",
    "            len(word) > 1 and word[-2].lower() == 'l'):\n",
    "            return word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        \"\"\"Apply the complete Porter2 stemming algorithm\"\"\"\n",
    "        if len(word) <= 2:\n",
    "            return word\n",
    "\n",
    "        # Convert to lowercase for processing but preserve original case pattern\n",
    "        original = word\n",
    "        word = word.lower()\n",
    "\n",
    "        # Mark initial y as consonant by converting to Y\n",
    "        if word.startswith('y'):\n",
    "            word = 'Y' + word[1:]\n",
    "\n",
    "        # Mark y as consonant when preceded by vowel\n",
    "        new_word = \"\"\n",
    "        for i, char in enumerate(word):\n",
    "            if char == 'y' and i > 0 and self.is_vowel(word, i - 1):\n",
    "                new_word += 'Y'\n",
    "            else:\n",
    "                new_word += char\n",
    "        word = new_word\n",
    "\n",
    "        # Apply stemming steps\n",
    "        word = self.step0(word)\n",
    "        word = self.step1a(word)\n",
    "        word = self.step1b(word)\n",
    "        word = self.step1c(word)\n",
    "        word = self.step2(word)\n",
    "        word = self.step3(word)\n",
    "        word = self.step4(word)\n",
    "        word = self.step5(word)\n",
    "\n",
    "        # Convert Y back to y\n",
    "        word = word.replace('Y', 'y')\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qR4Zuox92HRf",
   "metadata": {
    "id": "qR4Zuox92HRf"
   },
   "source": [
    "## text_stemmer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "TCN-PKhx2PwR",
   "metadata": {
    "id": "TCN-PKhx2PwR"
   },
   "outputs": [],
   "source": [
    "stemmer = Porter2Stemmer()\n",
    "def text_stemmer(text):\n",
    "    text = text if isinstance(text, str) else str(text)\n",
    "\n",
    "    words = text.strip().split()\n",
    "\n",
    "    # If it's a single word, stem and return it\n",
    "    if len(words) == 1:\n",
    "        return stemmer.stem(words[0])\n",
    "\n",
    "    # If it's a sentence, stem each word\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zPYP8sVVdfEh",
   "metadata": {
    "id": "zPYP8sVVdfEh"
   },
   "source": [
    "## build_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c5483",
   "metadata": {
    "id": "be3c5483"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocabulary(dataset):\n",
    "    word_counter = Counter()\n",
    "    for sentence in dataset:\n",
    "        words = Text_Processing(sentence).split()\n",
    "        word_counter.update(words)\n",
    "\n",
    "    sorted_words = sorted(word_counter.items(), key=lambda x: x[1])\n",
    "\n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(sorted_words)}\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def stabilize_vocab_stemming(vocab, stemmer):\n",
    "    prev_length = -1\n",
    "    while prev_length != len(vocab):\n",
    "        prev_length = len(vocab)\n",
    "        vocab = [stemmer.stem(word) for word in vocab]\n",
    "        vocab = list(dict.fromkeys(vocab))  # remove duplicates\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ICogm_bnfKla",
   "metadata": {
    "id": "ICogm_bnfKla"
   },
   "source": [
    "## remove unwanted samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e1348a",
   "metadata": {
    "id": "82e1348a"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def remove_samples_with_uncommon_chars(df, col_list=['question1', 'question2']):\n",
    "    allowed_pattern = re.compile(r'^[a-zA-Z0-9\\s!\"#$%&\\'()*+,\\-./:;<=>?@\\\\^_`{|}~]*$')\n",
    "    def is_clean(text):\n",
    "        text = str(text)  # Ensure it's a string\n",
    "        return bool(allowed_pattern.fullmatch(text))\n",
    "    mask = df[col_list].map(is_clean).all(axis=1)\n",
    "    return df[mask].reset_index(drop=True)\n",
    "\n",
    "def remove_short_samples(df, col_list=['question1', 'question2'], min_words=6):\n",
    "    def word_count(text):\n",
    "        return len(str(text).split())\n",
    "    mask = df[col_list].map(word_count).gt(min_words - 1).all(axis=1)\n",
    "    return df[mask].reset_index(drop=True)\n",
    "\n",
    "def remove_rare_word_samples(df, col_list=['question1', 'question2'], min_freq=5):\n",
    "    all_words = []\n",
    "    for col in col_list:\n",
    "        all_words.extend(' '.join(df[col].astype(str)).split())\n",
    "    word_freq = Counter(all_words)\n",
    "    def has_only_frequent_words(text):\n",
    "        words = str(text).split()\n",
    "        return all(word_freq[word] >= min_freq for word in words)\n",
    "    mask = df[col_list].map(has_only_frequent_words).all(axis=1)\n",
    "    return df[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7BFL4xfHdpUI",
   "metadata": {
    "id": "7BFL4xfHdpUI"
   },
   "source": [
    "## processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700331bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2466,
     "status": "ok",
     "timestamp": 1750798435570,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "700331bb",
    "outputId": "882f3fff-7a4e-4127-9907-5883f6e2fff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404288\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('semantic_web.csv')\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b51d69f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1268,
     "status": "ok",
     "timestamp": 1750798436841,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "5b51d69f",
    "outputId": "d7f79cac-016e-4c37-b066-5ad92c944238"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394624"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncommon = remove_samples_with_uncommon_chars(dataset)\n",
    "len(uncommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31fm8CyKfm5p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1750798437881,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "31fm8CyKfm5p",
    "outputId": "f5f19fa4-f37b-4e1a-9fbe-a577d3f839f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372573"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_samples = remove_short_samples(uncommon, min_words=5)\n",
    "len(short_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5Nhu-Uc7gbfK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6955,
     "status": "ok",
     "timestamp": 1750798444846,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "5Nhu-Uc7gbfK",
    "outputId": "5d72f6dc-50f4-47f8-c373-fa65f80a1a80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72914"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rare_words = remove_rare_word_samples(short_samples, min_freq=100)\n",
    "len(rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "PEQtL7WOg5GV",
   "metadata": {
    "id": "PEQtL7WOg5GV"
   },
   "outputs": [],
   "source": [
    "questions1 = rare_words['question1']\n",
    "\n",
    "questions2 = rare_words['question2']\n",
    "\n",
    "is_duplicate = rare_words['is_duplicate']\n",
    "\n",
    "new_dataset = pd.DataFrame({\n",
    "    'question1': questions1,\n",
    "    'question2': questions2,\n",
    "    'is_duplicate': is_duplicate\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "new_dataset.to_csv(\"processed_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1v__-TCD1Oda",
   "metadata": {
    "id": "1v__-TCD1Oda"
   },
   "source": [
    "## building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PEnrOp9FiN_U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1750801637460,
     "user": {
      "displayName": "IBRAHIM ALAHMAR",
      "userId": "07885065884680772489"
     },
     "user_tz": -180
    },
    "id": "PEnrOp9FiN_U",
    "outputId": "a7896138-b8b2-4c14-fbb3-5bfd0c92d951"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72914"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('processed_dataset.csv')\n",
    "print(len(dataset))\n",
    "Quest1 = dataset['question1']\n",
    "Quest2 = dataset['question2']\n",
    "Is_duplicate = dataset['is_duplicate']\n",
    "combined_dataset = list(Quest1) + list(Quest2)\n",
    "vocabulary = build_vocabulary(combined_dataset)\n",
    "print(len(vocabulary))\n",
    "vocabulary = stabilize_vocab_stemming(vocabulary, stemmer)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CdEs_zzU1btl",
   "metadata": {
    "id": "CdEs_zzU1btl"
   },
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2zie3hDs-99X",
   "metadata": {
    "id": "2zie3hDs-99X"
   },
   "outputs": [],
   "source": [
    "def bag_of_words_vectorizer(vocabulary, sentence):\n",
    "    # Tokenize the sentence\n",
    "    sentence = Text_Processing(sentence)\n",
    "    sentence = stemmer.stem(sentence)\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Initialize binary BoW vector\n",
    "    bow_vector = [0] * len(vocabulary)\n",
    "\n",
    "    # Set 1 if the word exists in the sentence\n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        bow_vector[idx] = 1 if word in words else 0\n",
    "\n",
    "    return bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M0GN3SzTJykP",
   "metadata": {
    "id": "M0GN3SzTJykP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.1, epochs=100):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = 0.0\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random.uniform(-0.01, 0.01, size=n_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(n_samples):\n",
    "                xi = X[i]\n",
    "                yi = y[i]\n",
    "                z = np.dot(self.weights, xi) + self.bias\n",
    "                y_hat = self._sigmoid(z)\n",
    "                error = y_hat - yi\n",
    "\n",
    "                # Gradient descent update\n",
    "                self.weights -= self.lr * error * xi\n",
    "                self.bias -= self.lr * error\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X)\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self._sigmoid(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        preds = self.predict(X)\n",
    "        accuracy = np.mean(preds == y)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d597be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabilize_vocab_stemming(vocab, stemmer):\n",
    "    prev_length = -1\n",
    "    while prev_length != len(vocab):\n",
    "        prev_length = len(vocab)\n",
    "        vocab = [stemmer.stem(word) for word in vocab]\n",
    "        vocab = list(dict.fromkeys(vocab))  # remove duplicates\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "_-2VzDRsUqMH",
   "metadata": {
    "id": "_-2VzDRsUqMH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for epoch 0 is 0.74875\n",
      "The accuracy for epoch 10 is 0.93375\n",
      "The accuracy for epoch 20 is 0.95875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m X_test, y_test = \u001b[38;5;28mzip\u001b[39m(*combined[split_idx:])\n\u001b[32m     25\u001b[39m model = LogisticRegression(lr=\u001b[32m0.1\u001b[39m, epochs=\u001b[32m100\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m accuracy = model.evaluate(X_test, y_test)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m xi, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, y):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m         correct = \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mint\u001b[39m(pred == true) \u001b[38;5;28;01mfor\u001b[39;00m pred, true \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, y))\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe accuracy for epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(y)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mLogisticRegression.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m >= \u001b[32m0.5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m X]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mLogisticRegression._predict_prob\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     z = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m + \u001b[38;5;28mself\u001b[39m.bias\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sigmoid(z)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     z = \u001b[38;5;28msum\u001b[39m(w * xi \u001b[38;5;28;01mfor\u001b[39;00m w, xi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.weights, x)) + \u001b[38;5;28mself\u001b[39m.bias\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sigmoid(z)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_size = 1000\n",
    "data_q1 = dataset['question1'][:sample_size]\n",
    "data_q2 = dataset['question2'][:sample_size]\n",
    "labels = dataset['is_duplicate'][:sample_size]\n",
    "\n",
    "combined_dataset = list(data_q1) + list(data_q2)\n",
    "vocab = build_vocabulary(combined_dataset)\n",
    "vocab = stabilize_vocab_stemming(vocab, stemmer)\n",
    "\n",
    "X = []\n",
    "for q1, q2 in zip(data_q1, data_q2):\n",
    "    v1 = bag_of_words_vectorizer(vocab, str(q1))\n",
    "    v2 = bag_of_words_vectorizer(vocab, str(q2))\n",
    "    X.append(v1 + v2)\n",
    "\n",
    "combined = list(zip(X, labels))\n",
    "random.shuffle(combined)\n",
    "split_idx = int(0.8 * len(combined))\n",
    "\n",
    "X_train, y_train = zip(*combined[:split_idx])\n",
    "X_test, y_test = zip(*combined[split_idx:])\n",
    "\n",
    "model = LogisticRegression(lr=0.1, epochs=100)\n",
    "model.fit(X_train, y_train)\n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xwlW4Hfxcd4F",
    "M0msY2jhc8hG",
    "v0QfC4x7dDhZ",
    "UX8vFm-vdLOe",
    "XbjtazL2lp8F",
    "qR4Zuox92HRf",
    "zPYP8sVVdfEh",
    "ICogm_bnfKla",
    "7BFL4xfHdpUI"
   ],
   "provenance": [
    {
     "file_id": "1mpct2jtoWO-QBNRmezllAkOD6ndfQg1G",
     "timestamp": 1750817538329
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
